{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import bigrams,trigrams, download \n",
    "from nltk.corpus import reuters\n",
    "from gensim.test.utils import datapath\n",
    "from gensim.corpora import WikiCorpus\n",
    "\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\tonoy\\appdata\\local\\pypoetry\\cache\\virtualenvs\\ai-bmuzw5yg-py3.7\\lib\\site-packages\\gensim\\utils.py:1268: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected %s; aliasing chunkize to chunkize_serial\" % entity)\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tonoy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\tonoy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object WikiCorpus.get_texts at 0x00000234ADCDBEC8>\n",
      "[['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'FROM', 'U', '.', 'S', '.-', 'JAPAN', 'RIFT', 'Mounting', 'trade', 'friction', 'between', 'the', 'U', '.', 'S', '.', 'And', 'Japan', 'has', 'raised', 'fears', 'among', 'many', 'of', 'Asia', \"'\", 's', 'exporting', 'nations', 'that', 'the', 'row', 'could', 'inflict', 'far', '-', 'reaching', 'economic', 'damage', ',', 'businessmen', 'and', 'officials', 'said', '.'], ['They', 'told', 'Reuter', 'correspondents', 'in', 'Asian', 'capitals', 'a', 'U', '.', 'S', '.', 'Move', 'against', 'Japan', 'might', 'boost', 'protectionist', 'sentiment', 'in', 'the', 'U', '.', 'S', '.', 'And', 'lead', 'to', 'curbs', 'on', 'American', 'imports', 'of', 'their', 'products', '.'], ...]\n"
     ]
    }
   ],
   "source": [
    "path_for_wiki_dataset = datapath('enwiki-latest-pages-articles1.xml-p000000010p000030302-shortened.bz2')\n",
    "wiki_sentences = WikiCorpus(path_for_wiki_dataset).get_texts()\n",
    "\n",
    "download('punkt')\n",
    "download('reuters')\n",
    "reuters_sentences  = reuters.sents()\n",
    "\n",
    "print(wiki_sentences)\n",
    "print(reuters_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_probabilities(sentence_model):\n",
    "  for next_word in sentence_model:\n",
    "    next_words = sentence_model[next_word]\n",
    "    total_word_count = float(sum(next_words.values()))\n",
    "    for previous_word in next_words:\n",
    "      next_words[previous_word] /= total_word_count\n",
    "    \n",
    "def calculate_sigle_word_probability(sentence_model, word_count):\n",
    "  for word in sentence_model:\n",
    "    sentence_model[word] /= word_count\n",
    "\n",
    "def convert_to_lower(word):\n",
    "  if isinstance(word, str):\n",
    "    return word.lower()\n",
    "  return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_model_1 = defaultdict(lambda:0)\n",
    "sentence_model_2 = defaultdict(lambda: defaultdict(lambda:0))\n",
    "sentence_model_3 = defaultdict(lambda: defaultdict(lambda:0))\n",
    "sentence_model_4 = defaultdict(lambda: set())\n",
    "sentence_model_5 = defaultdict(lambda: set())\n",
    "\n",
    "def calculate_word_count(sentence_model_1, sentence_model_2, sentence_model_3, sentences):\n",
    "  word_count = 0\n",
    "  for sentence in sentences:\n",
    "    for word in sentence:\n",
    "      word_count += 1\n",
    "      sentence_model_1[word] += 1\n",
    "    for previous_word_2, previous_word_1, next_word in trigrams(sentence, pad_right=True, pad_left=True):\n",
    "      previous_word_1 = convert_to_lower(previous_word_1)\n",
    "      previous_word_2 = convert_to_lower(previous_word_2)\n",
    "        \n",
    "      next_word = convert_to_lower(next_word)\n",
    "      sentence_model_2[next_word][previous_word_1] += 1\n",
    "      sentence_model_3[next_word][previous_word_2] += 1\n",
    "      sentence_model_4[previous_word_1].add(next_word)\n",
    "      sentence_model_5[previous_word_2].add(next_word)\n",
    "      \n",
    "  return word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "452944\n"
     ]
    }
   ],
   "source": [
    "wiki_word_count = calculate_word_count(sentence_model_1,sentence_model_2,sentence_model_3,wiki_sentences)\n",
    "print(wiki_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'FROM', 'U', '.', 'S', '.-', 'JAPAN', 'RIFT', 'Mounting', 'trade', 'friction', 'between', 'the', 'U', '.', 'S', '.', 'And', 'Japan', 'has', 'raised', 'fears', 'among', 'many', 'of', 'Asia', \"'\", 's', 'exporting', 'nations', 'that', 'the', 'row', 'could', 'inflict', 'far', '-', 'reaching', 'economic', 'damage', ',', 'businessmen', 'and', 'officials', 'said', '.'], ['They', 'told', 'Reuter', 'correspondents', 'in', 'Asian', 'capitals', 'a', 'U', '.', 'S', '.', 'Move', 'against', 'Japan', 'might', 'boost', 'protectionist', 'sentiment', 'in', 'the', 'U', '.', 'S', '.', 'And', 'lead', 'to', 'curbs', 'on', 'American', 'imports', 'of', 'their', 'products', '.'], ...]\n"
     ]
    }
   ],
   "source": [
    "reuters_word_count = calculate_word_count(sentence_model_1,sentence_model_2,sentence_model_3,reuters_sentences)\n",
    "print(reuters_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_word = wiki_word_count + reuters_word_count\n",
    "calculate_sigle_word_probability(sentence_model_1,total_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_probability_words = []\n",
    "def make_word_suggestion_by_trigram(previous_word_2, previous_word_1):\n",
    "  for next_word in sentence_model_4[previous_word_1] and sentence_model_5[previous_word_2]:\n",
    "    naive_bias_trigram_weight = sentence_model_1[next_word] * sentence_model_2[next_word][previous_word_1] * sentence_model_3[next_word][previous_word_2]\n",
    "    max_probability_words.append((next_word,naive_bias_trigram_weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('and', 0.3644483248928979) ('seventh', 0.0) ('meehan', 0.0) ('street', 0.0) ('frank', 0.0) ('by', 0.0) ('empire', 0.0) ('fosdick', 0.0) ('released', 0.0) (',', 0.0) ('russell', 0.0) ('mon', 0.0) ('victor', 0.0) ('for', 0.0) ('told', 0.0)\n"
     ]
    }
   ],
   "source": [
    "make_word_suggestion_by_trigram('harry','potter')\n",
    "max_probability_words.sort(key=lambda o:o[1],reverse=True)\n",
    "print(*max_probability_words[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_word(word_list):\n",
    "    make_word_suggestion_by_trigram(text[0].lower(), text[1].lower())\n",
    "    max_probability_words.sort(key=lambda o:o[1], reverse=True)\n",
    "    print(*max_probability_words[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter comma separated word(max 2):  linux,windows\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', 1.8677716744538864) ('of', 1.0056208745637372) ('it', 0.6233747235908828) (',', 0.3661503656397534) ('and', 0.3644483248928979) ('to', 0.3445965496413984) ('to', 0.3445965496413984) ('and', 0.16400174620180408) ('in', 0.1613934837600012) ('for', 0.15843745299262466) ('said', 0.0699400743653803) ('is', 0.04701680558232564) ('with', 0.04191160336378453) ('and', 0.03644483248928979) ('as', 0.011532936098490197) ('he', 0.004660831580308032) ('has', 0.0027172850518041402) ('down', 0.000443450616207752) ('nothing', 5.0141200380337106e-05) ('assume', 2.0700495569863942e-05)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter comma separated word(max 2):  harry,potter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', 1.8677716744538864) ('of', 1.0056208745637372) ('it', 0.6233747235908828) (',', 0.3661503656397534) ('and', 0.3644483248928979) ('and', 0.3644483248928979) ('to', 0.3445965496413984) ('to', 0.3445965496413984) ('and', 0.16400174620180408) ('in', 0.1613934837600012) ('for', 0.15843745299262466) ('said', 0.0699400743653803) ('is', 0.04701680558232564) ('with', 0.04191160336378453) ('and', 0.03644483248928979) ('as', 0.011532936098490197) ('he', 0.004660831580308032) ('has', 0.0027172850518041402) ('down', 0.000443450616207752) ('nothing', 5.0141200380337106e-05)\n"
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "    text = input(\"Enter comma separated word(max 2): \")\n",
    "    if text == \"stop\":\n",
    "        print(\"....Terminated....\")\n",
    "        break\n",
    "    else:\n",
    "        try:\n",
    "            text = text.split(\",\")\n",
    "            suggest_word(text)\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
